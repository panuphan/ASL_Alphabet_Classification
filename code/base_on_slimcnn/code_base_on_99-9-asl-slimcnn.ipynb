{"cells":[{"cell_type":"markdown","metadata":{"_uuid":"4d3a6f308a716b7346984f65bb3d7b67a7d315d6"},"outputs":[],"source":["# Data Preparation"]},{"cell_type":"code","metadata":{},"outputs":[],"source":["import os, cv2, skimage\n","from skimage.transform import resize\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.utils.np_utils import to_categorical\n","from keras.utils import print_summary, plot_model\n","from keras.layers import Conv2D, Dense, Dropout, Flatten, MaxPool2D,Input, Add, GlobalAveragePooling2D, DepthwiseConv2D, BatchNormalization, LeakyReLU\n","from keras.models import Model , load_model , Sequential\n","from keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau\n","from glob import glob\n","train_dir = 'data/input/NewData/train/'\n","test_dir = 'data/input/NewData/test/'\n","CLASSES = [os.path.basename(folder) for folder in glob(train_dir + '/*')]\n","CLASSES.sort()\n","# CLASSES.append(\"other\")\n","\n","batch_size = 64\n","imageSize = 64\n","target_dims = (imageSize, imageSize, 3)\n","num_classes = 25"]},{"cell_type":"code","metadata":{},"outputs":[],"source":["def get_data(folder,limit=10):\n","    \"\"\"\n","    Load the data and labels from the given folder.\n","    \"\"\"\n","    train_len = limit*len(os.listdir(folder))\n","    print(\"num_datas:\",train_len)\n","    X = np.empty((train_len, imageSize, imageSize, 3), dtype=np.float32)\n","    y = np.empty((train_len,), dtype=np.int)\n","    cnt = 0\n","\n","    for folderName in os.listdir(folder):\n","        if not folderName.startswith('.'):\n","            if folderName in ['A']:\n","                label = 0\n","            elif folderName in ['B']:\n","                label = 1\n","            elif folderName in ['C']:\n","                label = 2\n","            elif folderName in ['D']:\n","                label = 3\n","            elif folderName in ['E']:\n","                label = 4\n","            elif folderName in ['F']:\n","                label = 5\n","            elif folderName in ['G']:\n","                label = 6\n","            elif folderName in ['H']:\n","                label = 7\n","            elif folderName in ['I']:\n","                label = 8\n","            # elif folderName in ['J']:\n","            #     label = 9\n","            elif folderName in ['K']:\n","                label = 10-1\n","            elif folderName in ['L']:\n","                label = 11-1\n","            elif folderName in ['M']:\n","                label = 12-1\n","            elif folderName in ['N']:\n","                label = 13-1\n","            elif folderName in ['O']:\n","                label = 14-1\n","            elif folderName in ['P']:\n","                label = 15-1\n","            elif folderName in ['Q']:\n","                label = 16-1\n","            elif folderName in ['R']:\n","                label = 17-1\n","            elif folderName in ['S']:\n","                label = 18-1\n","            elif folderName in ['T']:\n","                label = 19-1\n","            elif folderName in ['U']:\n","                label = 20-1\n","            elif folderName in ['V']:\n","                label = 21-1\n","            elif folderName in ['W']:\n","                label = 22-1\n","            elif folderName in ['X']:\n","                label = 23-1\n","            elif folderName in ['Y']:\n","                label = 24-1\n","            # elif folderName in ['Z']:\n","            #     label = 25\n","            # elif folderName in ['del']:\n","            #     label = 26\n","            # elif folderName in ['nothing']:\n","            #     label = 27\n","            # elif folderName in ['space']:\n","            #     label = 28           \n","            else:\n","                label = 25\n","            limit = len(os.listdir(folder + folderName)) if limit is None else limit\n","            # print(f\"folder{label}: {limit}\")\n","            for iter,image_filename in enumerate(os.listdir(folder + folderName)):\n","                if(iter < limit):\n","                    img_file = cv2.imread(folder + folderName + '/' + image_filename)\n","                    if img_file is not None:\n","                        img_file = skimage.transform.resize(img_file, (imageSize, imageSize, 3))\n","                        img_arr = np.asarray(img_file).reshape((-1, imageSize, imageSize, 3))\n","                        \n","                        X[cnt] = img_arr\n","                        y[cnt] = label\n","                        # print(y[cnt])\n","                        cnt += 1\n","                    # X.append(img_arr)\n","                    # y.append(label)]\n","                else: continue\n","    X = np.asarray(X)\n","    y = np.asarray(y)\n","    return X,y\n"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"num_datas: 7200\n"},{"data":{"text/plain":"((5760, 64, 64, 3), (5760, 25), (1440, 64, 64, 3), (1440, 25))"},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["X_train, y_train = get_data(train_dir,300) \n","\n","X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2) \n","\n","# Encode labels to hot vectors (ex : 2 -> [0,0,1,0,0,0,0,0,0,0])\n","y_trainHot = to_categorical(y_train,num_classes=num_classes)\n","y_testHot = to_categorical(y_test,num_classes=num_classes)\n","\n","X_train.shape, y_trainHot.shape, X_test.shape, y_testHot.shape\n"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/plain":"((5760, 64, 64, 3), (1440, 64, 64, 3), (5760,), (1440,))"},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["X_train.shape, X_test.shape, y_train.shape, y_test.shape "]},{"cell_type":"markdown","metadata":{"_uuid":"c596b3cf8e2c68d6e62adf5277da7a1ca9077c6b"},"outputs":[],"source":["# Data Augmentation"]},{"cell_type":"code","metadata":{},"outputs":[],"source":["train_image_generator = ImageDataGenerator(\n","    samplewise_center=True,\n","    samplewise_std_normalization=True,\n","    rotation_range=20,\n","    width_shift_range=0.2,\n","    height_shift_range=0.2,\n","    horizontal_flip=True\n",")\n","\n","val_image_generator = ImageDataGenerator(\n","    samplewise_center=True,\n","    samplewise_std_normalization=True,\n",")\n","\n","\n","# test_image_generator = ImageDataGenerator(\n","#     samplewise_center=True,\n","#     samplewise_std_normalization=True,\n","# )\n","\n","\n","train_generator = train_image_generator.flow(x=X_train, y=y_trainHot, batch_size=batch_size, shuffle=True)\n","val_generator = val_image_generator.flow(x=X_test, y=y_testHot, batch_size=batch_size, shuffle=False)\n","# test_generator = test_image_generator.flow_from_directory(train_dir,target_size=(imageSize, imageSize),color_mode=\"rgb\",batch_size=32,class_mode=None,shuffle=False)\n","\n","# filenames = test_generator.filenames\n","# nb_samples = len(filenames)\n","# print(\"nb_samples:\",nb_samples)\n","# predict = model.predict_generator(test_generator,steps = nb_samples)"]},{"cell_type":"markdown","metadata":{"_uuid":"9801290b12bff9ca77ef29d860681e413f94497c"},"outputs":[],"source":["# Model SlimCNN"]},{"cell_type":"code","metadata":{},"outputs":[],"source":["def slimCNN():\n","    inputs = Input(shape=target_dims)\n","    net = Conv2D(32, kernel_size=3, strides=1, padding=\"same\")(inputs)\n","    net = LeakyReLU()(net)\n","    net = Conv2D(32, kernel_size=3, strides=1, padding=\"same\")(net)\n","    net = LeakyReLU()(net)\n","    net = Conv2D(32, kernel_size=3, strides=2, padding=\"same\")(net)\n","    net = LeakyReLU()(net)\n","\n","    net = Conv2D(32, kernel_size=3, strides=1, padding=\"same\")(net)\n","    net = LeakyReLU()(net)\n","    net = Conv2D(32, kernel_size=3, strides=1, padding=\"same\")(net)\n","    net = LeakyReLU()(net)\n","    net = Conv2D(32, kernel_size=3, strides=2, padding=\"same\")(net)\n","    net = LeakyReLU()(net)\n","\n","    shortcut = net\n","\n","    net = DepthwiseConv2D(kernel_size=3, strides=1, padding='same', kernel_initializer='he_normal')(net)\n","    net = BatchNormalization(axis=3)(net)\n","    net = LeakyReLU()(net)\n","    net = Conv2D(filters=32, kernel_size=1, strides=1, padding='same', kernel_initializer='he_normal')(net)\n","    net = BatchNormalization(axis=3)(net)\n","    net = LeakyReLU()(net)\n","\n","    net = DepthwiseConv2D(kernel_size=3, strides=1, padding='same', kernel_initializer='he_normal')(net)\n","    net = BatchNormalization(axis=3)(net)\n","    net = LeakyReLU()(net)\n","    net = Conv2D(filters=32, kernel_size=1, strides=1, padding='same', kernel_initializer='he_normal')(net)\n","    net = BatchNormalization(axis=3)(net)\n","    net = LeakyReLU()(net)\n","\n","    net = Add()([net, shortcut])\n","\n","    net = GlobalAveragePooling2D()(net)\n","    net = Dropout(0.2)(net)\n","\n","    net = Dense(128, activation='relu')(net)\n","    outputs = Dense(num_classes, activation='softmax')(net)\n","\n","    model = Model(inputs=inputs, outputs=outputs)\n","    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[\"accuracy\"])\n","\n","    return model\n","\n","model=slimCNN()\n","model.summary()\n","# plot_model(model, to_file=\"model.png\", show_shapes=True, show_layer_names=True)"]},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["# Model AlexNet"]},{"cell_type":"code","metadata":{},"outputs":[],"source":["def AlexNet():\n","    model = Sequential()\n","    model.add(Conv2D(64, kernel_size=4, strides=1, activation='relu', input_shape=target_dims))\n","    model.add(Conv2D(64, kernel_size=4, strides=2, activation='relu'))\n","    model.add(Dropout(0.5))\n","    model.add(Conv2D(128, kernel_size=4, strides=1, activation='relu'))\n","    model.add(Conv2D(128, kernel_size=4, strides=2, activation='relu'))\n","    model.add(Dropout(0.5))\n","    model.add(Conv2D(256, kernel_size=4, strides=1, activation='relu'))\n","    model.add(Conv2D(256, kernel_size=4, strides=2, activation='relu'))\n","    model.add(Flatten())\n","    model.add(Dropout(0.5))\n","    model.add(Dense(512, activation='relu'))\n","    outputs = Dense(num_classes, activation='softmax')\n","\n","    model = Model(inputs=inputs, outputs=outputs)\n","    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[\"accuracy\"])\n","    # model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","    return model\n","\n","model=AlexNet()\n","model.summary()\n","# plot_model(model, to_file=\"model.png\", show_shapes=True, show_layer_names=True)"]},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["# Model custom"]},{"cell_type":"code","metadata":{},"outputs":[],"source":["from keras import regularizers\n","from keras.losses import categorical_crossentropy\n","def model_custom():\n","    model = Sequential()\n","    \n","    model.add(Conv2D(16, kernel_size = [3,3], padding = 'same', activation = 'relu', input_shape = (64,64,3)))\n","    model.add(Conv2D(32, kernel_size = [3,3], padding = 'same', activation = 'relu'))\n","    model.add(MaxPool2D(pool_size = [3,3]))\n","    \n","    model.add(Conv2D(32, kernel_size = [3,3], padding = 'same', activation = 'relu'))\n","    model.add(Conv2D(64, kernel_size = [3,3], padding = 'same', activation = 'relu'))\n","    model.add(MaxPool2D(pool_size = [3,3]))\n","    \n","    model.add(Conv2D(128, kernel_size = [3,3], padding = 'same', activation = 'relu'))\n","    model.add(Conv2D(256, kernel_size = [3,3], padding = 'same', activation = 'relu'))\n","    model.add(MaxPool2D(pool_size = [3,3]))\n","    \n","    model.add(BatchNormalization())\n","    \n","    model.add(Flatten())\n","    model.add(Dropout(0.5))\n","    model.add(Dense(512, activation = 'relu', kernel_regularizer = regularizers.l2(0.001)))\n","    model.add(Dense(num_classes, activation = 'softmax'))\n","    \n","    model.compile(optimizer = 'adam', loss = categorical_crossentropy, metrics = [\"accuracy\"])\n","    return model\n","\n","model = model_custom()\n","model.summary()"]},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":["#Model VGG16"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"Model: \"sequential_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nvgg16 (Model)                (None, 2, 2, 512)         14714688  \n_________________________________________________________________\nflatten_3 (Flatten)          (None, 2048)              0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 25)                51225     \n=================================================================\nTotal params: 14,765,913\nTrainable params: 14,765,913\nNon-trainable params: 0\n_________________________________________________________________\n"}],"source":["from keras.applications.vgg16 import VGG16\n","from keras.optimizers import SGD, RMSprop, Adam, Adagrad, Adadelta, RMSprop\n","def vgg16():\n","    vgg_base = VGG16(weights='data\\input\\pretrain-model/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5',include_top=False,input_shape=target_dims)\n","\n","    #initiate a model\n","    model = Sequential()\n","\n","    #Add the VGG base model\n","    model.add(vgg_base)\n","\n","    #Add new layers\n","    model.add(Flatten())\n","    model.add(Dense(num_classes, activation='softmax'))\n","\n","    #summary of the model\n","    #Adam=Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n","    sgd = SGD(lr=0.001)\n","    model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n","    return model\n","model = vgg16()\n","model.summary()\n"]},{"cell_type":"markdown","metadata":{"_uuid":"16fa58a2542584cc8597564daa8446a04aa92560"},"outputs":[],"source":["# Training"]},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["## fit"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"WARNING:tensorflow:From C:\\Users\\yut\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n\nTrain on 5760 samples, validate on 1440 samples\nEpoch 1/100\n - 11s - loss: 2.8119 - accuracy: 0.2073 - val_loss: 2.0111 - val_accuracy: 0.4007\nEpoch 2/100\n - 7s - loss: 1.2397 - accuracy: 0.6234 - val_loss: 0.5740 - val_accuracy: 0.8382\nEpoch 3/100\n - 7s - loss: 0.3906 - accuracy: 0.8938 - val_loss: 0.1162 - val_accuracy: 0.9799\nEpoch 4/100\n - 7s - loss: 0.0695 - accuracy: 0.9915 - val_loss: 0.0289 - val_accuracy: 0.9993\nEpoch 5/100\n - 7s - loss: 0.0190 - accuracy: 0.9997 - val_loss: 0.0145 - val_accuracy: 1.0000\nEpoch 6/100\n - 7s - loss: 0.0106 - accuracy: 0.9998 - val_loss: 0.0091 - val_accuracy: 1.0000\nEpoch 7/100\n - 7s - loss: 0.0073 - accuracy: 0.9998 - val_loss: 0.0066 - val_accuracy: 1.0000\nEpoch 8/100\n - 7s - loss: 0.0055 - accuracy: 0.9998 - val_loss: 0.0056 - val_accuracy: 1.0000\nEpoch 9/100\n - 7s - loss: 0.0044 - accuracy: 0.9998 - val_loss: 0.0043 - val_accuracy: 1.0000\nEpoch 10/100\n - 7s - loss: 0.0036 - accuracy: 0.9998 - val_loss: 0.0040 - val_accuracy: 1.0000\nEpoch 11/100\n - 7s - loss: 0.0031 - accuracy: 0.9998 - val_loss: 0.0031 - val_accuracy: 1.0000\nEpoch 12/100\n - 7s - loss: 0.0027 - accuracy: 0.9998 - val_loss: 0.0029 - val_accuracy: 1.0000\nEpoch 13/100\n - 7s - loss: 0.0024 - accuracy: 0.9998 - val_loss: 0.0024 - val_accuracy: 1.0000\nEpoch 14/100\n - 7s - loss: 0.0021 - accuracy: 0.9998 - val_loss: 0.0021 - val_accuracy: 1.0000\nEpoch 15/100\n - 7s - loss: 0.0019 - accuracy: 0.9998 - val_loss: 0.0019 - val_accuracy: 1.0000\nEpoch 16/100\n - 7s - loss: 0.0017 - accuracy: 0.9998 - val_loss: 0.0018 - val_accuracy: 1.0000\nEpoch 17/100\n - 7s - loss: 0.0016 - accuracy: 0.9998 - val_loss: 0.0016 - val_accuracy: 1.0000\nEpoch 18/100\n - 7s - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0014 - val_accuracy: 1.0000\nEpoch 19/100\n - 7s - loss: 0.0013 - accuracy: 0.9998 - val_loss: 0.0019 - val_accuracy: 1.0000\nEpoch 20/100\n - 7s - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000\nEpoch 21/100\n - 7s - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.0011 - val_accuracy: 1.0000\nEpoch 22/100\n - 7s - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.0011 - val_accuracy: 1.0000\nEpoch 23/100\n - 7s - loss: 9.8671e-04 - accuracy: 1.0000 - val_loss: 0.0010 - val_accuracy: 1.0000\nEpoch 24/100\n - 7s - loss: 9.8117e-04 - accuracy: 0.9998 - val_loss: 9.5803e-04 - val_accuracy: 1.0000\nEpoch 25/100\n - 7s - loss: 9.2704e-04 - accuracy: 0.9998 - val_loss: 9.3466e-04 - val_accuracy: 1.0000\nEpoch 26/100\n - 7s - loss: 7.9390e-04 - accuracy: 1.0000 - val_loss: 8.6702e-04 - val_accuracy: 1.0000\nEpoch 27/100\n - 7s - loss: 8.3956e-04 - accuracy: 0.9998 - val_loss: 8.3815e-04 - val_accuracy: 1.0000\nEpoch 28/100\n - 7s - loss: 7.5370e-04 - accuracy: 1.0000 - val_loss: 7.8747e-04 - val_accuracy: 1.0000\nEpoch 29/100\n - 7s - loss: 6.9398e-04 - accuracy: 1.0000 - val_loss: 9.6248e-04 - val_accuracy: 1.0000\nEpoch 30/100\n - 7s - loss: 6.9291e-04 - accuracy: 1.0000 - val_loss: 8.4333e-04 - val_accuracy: 1.0000\nEpoch 31/100\n - 7s - loss: 6.5430e-04 - accuracy: 1.0000 - val_loss: 7.0153e-04 - val_accuracy: 1.0000\nEpoch 32/100\n - 7s - loss: 6.1237e-04 - accuracy: 1.0000 - val_loss: 6.7241e-04 - val_accuracy: 1.0000\nEpoch 33/100\n - 7s - loss: 5.5421e-04 - accuracy: 1.0000 - val_loss: 6.2954e-04 - val_accuracy: 1.0000\nEpoch 34/100\n - 7s - loss: 5.7104e-04 - accuracy: 1.0000 - val_loss: 6.1314e-04 - val_accuracy: 1.0000\nEpoch 35/100\n - 7s - loss: 5.3537e-04 - accuracy: 1.0000 - val_loss: 5.9549e-04 - val_accuracy: 1.0000\nEpoch 36/100\n - 7s - loss: 5.0527e-04 - accuracy: 1.0000 - val_loss: 5.6689e-04 - val_accuracy: 1.0000\nEpoch 37/100\n - 7s - loss: 4.7483e-04 - accuracy: 1.0000 - val_loss: 5.4004e-04 - val_accuracy: 1.0000\nEpoch 38/100\n - 7s - loss: 4.8167e-04 - accuracy: 1.0000 - val_loss: 5.3241e-04 - val_accuracy: 1.0000\nEpoch 39/100\n - 7s - loss: 4.5372e-04 - accuracy: 1.0000 - val_loss: 5.1108e-04 - val_accuracy: 1.0000\nEpoch 40/100\n - 7s - loss: 4.3908e-04 - accuracy: 1.0000 - val_loss: 5.0144e-04 - val_accuracy: 1.0000\nEpoch 41/100\n - 7s - loss: 4.1857e-04 - accuracy: 1.0000 - val_loss: 5.2086e-04 - val_accuracy: 1.0000\nEpoch 42/100\n - 7s - loss: 4.0269e-04 - accuracy: 1.0000 - val_loss: 4.6271e-04 - val_accuracy: 1.0000\nEpoch 43/100\n - 7s - loss: 3.8809e-04 - accuracy: 1.0000 - val_loss: 5.0078e-04 - val_accuracy: 1.0000\nEpoch 44/100\n - 7s - loss: 3.8400e-04 - accuracy: 1.0000 - val_loss: 4.3682e-04 - val_accuracy: 1.0000\nEpoch 45/100\n - 7s - loss: 3.6530e-04 - accuracy: 1.0000 - val_loss: 4.6484e-04 - val_accuracy: 1.0000\nEpoch 46/100\n - 7s - loss: 3.5860e-04 - accuracy: 1.0000 - val_loss: 4.3663e-04 - val_accuracy: 1.0000\nEpoch 47/100\n - 7s - loss: 3.3635e-04 - accuracy: 1.0000 - val_loss: 3.9590e-04 - val_accuracy: 1.0000\nEpoch 48/100\n - 7s - loss: 3.4067e-04 - accuracy: 1.0000 - val_loss: 3.8949e-04 - val_accuracy: 1.0000\nEpoch 49/100\n - 7s - loss: 3.2672e-04 - accuracy: 1.0000 - val_loss: 3.8479e-04 - val_accuracy: 1.0000\nEpoch 50/100\n - 7s - loss: 3.1909e-04 - accuracy: 1.0000 - val_loss: 3.7093e-04 - val_accuracy: 1.0000\nEpoch 51/100\n - 7s - loss: 3.0179e-04 - accuracy: 1.0000 - val_loss: 3.6018e-04 - val_accuracy: 1.0000\nEpoch 52/100\n - 7s - loss: 2.9982e-04 - accuracy: 1.0000 - val_loss: 3.5527e-04 - val_accuracy: 1.0000\nEpoch 53/100\n - 7s - loss: 2.8838e-04 - accuracy: 1.0000 - val_loss: 3.4218e-04 - val_accuracy: 1.0000\nEpoch 54/100\n - 7s - loss: 2.8413e-04 - accuracy: 1.0000 - val_loss: 3.5016e-04 - val_accuracy: 1.0000\nEpoch 55/100\n - 7s - loss: 2.7144e-04 - accuracy: 1.0000 - val_loss: 3.2618e-04 - val_accuracy: 1.0000\nEpoch 56/100\n - 7s - loss: 2.6932e-04 - accuracy: 1.0000 - val_loss: 3.1674e-04 - val_accuracy: 1.0000\nEpoch 57/100\n - 7s - loss: 2.6699e-04 - accuracy: 1.0000 - val_loss: 3.1308e-04 - val_accuracy: 1.0000\nEpoch 58/100\n - 7s - loss: 2.5413e-04 - accuracy: 1.0000 - val_loss: 3.1439e-04 - val_accuracy: 1.0000\nEpoch 59/100\n - 7s - loss: 2.4933e-04 - accuracy: 1.0000 - val_loss: 2.9841e-04 - val_accuracy: 1.0000\nEpoch 60/100\n - 7s - loss: 2.4538e-04 - accuracy: 1.0000 - val_loss: 3.0550e-04 - val_accuracy: 1.0000\nEpoch 61/100\n - 7s - loss: 2.4182e-04 - accuracy: 1.0000 - val_loss: 2.9058e-04 - val_accuracy: 1.0000\nEpoch 62/100\n - 7s - loss: 2.3442e-04 - accuracy: 1.0000 - val_loss: 2.9112e-04 - val_accuracy: 1.0000\nEpoch 63/100\n - 7s - loss: 2.2992e-04 - accuracy: 1.0000 - val_loss: 2.7717e-04 - val_accuracy: 1.0000\nEpoch 64/100\n - 7s - loss: 2.2288e-04 - accuracy: 1.0000 - val_loss: 2.6818e-04 - val_accuracy: 1.0000\nEpoch 65/100\n - 7s - loss: 2.2071e-04 - accuracy: 1.0000 - val_loss: 2.6313e-04 - val_accuracy: 1.0000\nEpoch 66/100\n - 7s - loss: 2.1759e-04 - accuracy: 1.0000 - val_loss: 2.5957e-04 - val_accuracy: 1.0000\nEpoch 67/100\n - 7s - loss: 2.1199e-04 - accuracy: 1.0000 - val_loss: 2.5439e-04 - val_accuracy: 1.0000\nEpoch 68/100\n - 7s - loss: 2.0804e-04 - accuracy: 1.0000 - val_loss: 2.4972e-04 - val_accuracy: 1.0000\nEpoch 69/100\n - 7s - loss: 2.0465e-04 - accuracy: 1.0000 - val_loss: 2.4626e-04 - val_accuracy: 1.0000\nEpoch 70/100\n - 7s - loss: 2.0018e-04 - accuracy: 1.0000 - val_loss: 2.4378e-04 - val_accuracy: 1.0000\nEpoch 71/100\n - 7s - loss: 1.9610e-04 - accuracy: 1.0000 - val_loss: 2.4314e-04 - val_accuracy: 1.0000\nEpoch 72/100\n - 7s - loss: 1.9274e-04 - accuracy: 1.0000 - val_loss: 2.3394e-04 - val_accuracy: 1.0000\nEpoch 73/100\n - 7s - loss: 1.8913e-04 - accuracy: 1.0000 - val_loss: 2.2959e-04 - val_accuracy: 1.0000\nEpoch 74/100\n - 7s - loss: 1.8638e-04 - accuracy: 1.0000 - val_loss: 2.2567e-04 - val_accuracy: 1.0000\nEpoch 75/100\n - 7s - loss: 1.8292e-04 - accuracy: 1.0000 - val_loss: 2.2262e-04 - val_accuracy: 1.0000\nEpoch 76/100\n - 7s - loss: 1.7939e-04 - accuracy: 1.0000 - val_loss: 2.1846e-04 - val_accuracy: 1.0000\nEpoch 77/100\n - 7s - loss: 1.7601e-04 - accuracy: 1.0000 - val_loss: 2.1846e-04 - val_accuracy: 1.0000\nEpoch 78/100\n - 7s - loss: 1.7352e-04 - accuracy: 1.0000 - val_loss: 2.1195e-04 - val_accuracy: 1.0000\nEpoch 79/100\n - 7s - loss: 1.6935e-04 - accuracy: 1.0000 - val_loss: 2.0744e-04 - val_accuracy: 1.0000\nEpoch 80/100\n - 7s - loss: 1.6874e-04 - accuracy: 1.0000 - val_loss: 2.0611e-04 - val_accuracy: 1.0000\nEpoch 81/100\n - 7s - loss: 1.6371e-04 - accuracy: 1.0000 - val_loss: 2.0122e-04 - val_accuracy: 1.0000\nEpoch 82/100\n - 7s - loss: 1.6286e-04 - accuracy: 1.0000 - val_loss: 2.0132e-04 - val_accuracy: 1.0000\nEpoch 83/100\n - 7s - loss: 1.5862e-04 - accuracy: 1.0000 - val_loss: 1.9520e-04 - val_accuracy: 1.0000\nEpoch 84/100\n - 7s - loss: 1.5813e-04 - accuracy: 1.0000 - val_loss: 1.9428e-04 - val_accuracy: 1.0000\nEpoch 85/100\n - 7s - loss: 1.5357e-04 - accuracy: 1.0000 - val_loss: 1.8959e-04 - val_accuracy: 1.0000\nEpoch 86/100\n - 7s - loss: 1.5307e-04 - accuracy: 1.0000 - val_loss: 1.8653e-04 - val_accuracy: 1.0000\nEpoch 87/100\n - 7s - loss: 1.5100e-04 - accuracy: 1.0000 - val_loss: 1.8472e-04 - val_accuracy: 1.0000\nEpoch 88/100\n - 7s - loss: 1.4863e-04 - accuracy: 1.0000 - val_loss: 1.8154e-04 - val_accuracy: 1.0000\nEpoch 89/100\n - 7s - loss: 1.4625e-04 - accuracy: 1.0000 - val_loss: 1.7920e-04 - val_accuracy: 1.0000\nEpoch 90/100\n - 7s - loss: 1.4388e-04 - accuracy: 1.0000 - val_loss: 1.7612e-04 - val_accuracy: 1.0000\nEpoch 91/100\n - 7s - loss: 1.4272e-04 - accuracy: 1.0000 - val_loss: 1.7539e-04 - val_accuracy: 1.0000\nEpoch 92/100\n - 7s - loss: 1.3991e-04 - accuracy: 1.0000 - val_loss: 1.7507e-04 - val_accuracy: 1.0000\nEpoch 93/100\n - 7s - loss: 1.3760e-04 - accuracy: 1.0000 - val_loss: 1.7180e-04 - val_accuracy: 1.0000\nEpoch 94/100\n - 7s - loss: 1.3678e-04 - accuracy: 1.0000 - val_loss: 1.6886e-04 - val_accuracy: 1.0000\nEpoch 95/100\n - 7s - loss: 1.3376e-04 - accuracy: 1.0000 - val_loss: 1.6559e-04 - val_accuracy: 1.0000\nEpoch 96/100\n - 7s - loss: 1.3295e-04 - accuracy: 1.0000 - val_loss: 1.6364e-04 - val_accuracy: 1.0000\nEpoch 97/100\n - 7s - loss: 1.3115e-04 - accuracy: 1.0000 - val_loss: 1.6225e-04 - val_accuracy: 1.0000\nEpoch 98/100\n - 7s - loss: 1.2915e-04 - accuracy: 1.0000 - val_loss: 1.5969e-04 - val_accuracy: 1.0000\nEpoch 99/100\n - 7s - loss: 1.2738e-04 - accuracy: 1.0000 - val_loss: 1.5949e-04 - val_accuracy: 1.0000\nEpoch 100/100\n - 7s - loss: 1.2554e-04 - accuracy: 1.0000 - val_loss: 1.5532e-04 - val_accuracy: 1.0000\n"}],"source":["from math import ceil\n","train_log=model.fit(X_train,y_trainHot, epochs=100,\n","    validation_data=(X_test,y_testHot),\n","    batch_size=batch_size,\n","    # steps_per_epoch=int(ceil(len(X_train)/batch_size)),\n","    # validation_steps=int(ceil(len(X_test)/batch_size)),\n","    use_multiprocessing=False,\n","    verbose=2,\n","    # callbacks=[\n","    #     # TensorBoard(log_dir='./logs/%s' % (start_time)),\n","    #     # ModelCheckpoint('./models/%s.h5' % (start_time), monitor='val_acc', verbose=1, save_best_only=True, mode='auto'),\n","    #     ReduceLROnPlateau(monitor='val_acc', factor=0.2, patience=5, verbose=2, mode='auto')]\n","        )"]},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["## fit_generator"]},{"cell_type":"code","metadata":{},"outputs":[],"source":["import datetime\n","start_time = datetime.datetime.now().strftime('%Y_%m_%d_%H_%M_%S')\n","\n","train_log=model.fit_generator(train_generator, epochs=100, validation_data=val_generator,\n","    steps_per_epoch=train_generator.__len__(),\n","    validation_steps=val_generator.__len__(),\n","    verbose=2,\n","    callbacks=[\n","        # TensorBoard(log_dir='./logs/%s' % (start_time)),\n","        # ModelCheckpoint('./models/%s.h5' % (start_time), monitor='val_acc', verbose=1, save_best_only=True, mode='auto'),\n","        ReduceLROnPlateau(monitor='val_acc', factor=0.2, patience=5, verbose=2, mode='auto')]\n","        )"]},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["# SAVE Model"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["import pickle\n","import time, datetime, pytz\n","\n","def get_datetime():\n","    dt = datetime.datetime.fromtimestamp(time.time()).astimezone(pytz.timezone('Asia/Bangkok'))\n","    return dt.strftime(\"%Y-%m-%d_%H.%M.%S\")\n","exec_time = get_datetime()\n","\n","MODEL_NAME = exec_time + '-model'\n","MODEL_DIR = 'model/myVGG16/'\n","MODEL_SAVE_DIR = MODEL_DIR + MODEL_NAME + '.h5'\n","MODEL_SAVE_WEIGHTS_DIR = MODEL_DIR + MODEL_NAME + '.weights.h5'\n","MODEL_SAVE_TRAIN_LOG_DIR = MODEL_DIR + MODEL_NAME + '-train-log.pickle'\n","model.save(MODEL_SAVE_DIR)\n","model.save_weights(MODEL_SAVE_WEIGHTS_DIR)\n","with open(MODEL_SAVE_TRAIN_LOG_DIR, 'wb') as file:\n","    pickle.dump(train_log, file)"]},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["# LOAD Model"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"Reloading old model, weights and training log from disk\nDone!\n"}],"source":["from pathlib import Path\n","def reload_model():\n","    old_model_file = Path(MODEL_SAVE_DIR)\n","    old_weight_file = Path(MODEL_SAVE_WEIGHTS_DIR)\n","    old_train_log_file = Path(MODEL_SAVE_TRAIN_LOG_DIR)\n","    if old_model_file.is_file() and old_weight_file.is_file() and old_train_log_file.is_file():\n","        print(\"Reloading old model, weights and training log from disk\")\n","        model = load_model(MODEL_SAVE_DIR)\n","        model.load_weights(MODEL_SAVE_WEIGHTS_DIR)\n","        with open(MODEL_SAVE_TRAIN_LOG_DIR, 'rb') as file:\n","            train_log = pickle.load(file)\n","        print(\"Done!\")\n","        return model, train_log\n","    else:\n","        print(\"Cannot reload the old model, weight and training log from\\n  * \\\"%s\\\"\\n  * \\\"%s\\\"\\n  * \\\"%s\\\"\" \n","              % (RELOAD_MODEL_DIR, RELOAD_MODEL_WEIGHTS_DIR, RELOAD_MODEL_TRAIN_LOG_DIR))\n","        print(\"Please check if the path is correct or not\")\n","        return None, None\n","\n","model, train_log = reload_model()"]},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["# evaluate"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"num_datas: 720\n720/720 [==============================] - 3s 5ms/step\ntest loss, test acc: [0.0028258773899864703, 1.0]\n"},{"data":{"text/plain":"(720, 64, 64, 3)"},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["\n","X_custom,y_custom=get_data(test_dir,30)#'data/input/NewData/test/'\n","y_customnHot = to_categorical(y_custom, num_classes=num_classes)\n","results = model.evaluate(X_custom,y_customnHot,batch_size=1)\n","print('test loss, test acc:', results)\n","X_custom.shape"]},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["# evaluate_generator"]},{"cell_type":"code","metadata":{},"outputs":[],"source":["results = model.evaluate_generator(val_generator)\n","print('test loss, test acc:', results)\n","\n","print('\\n# Generate predictions for 3 samples')\n","predictions = model.predict_generator(val_generator)\n","print('predictions:', np.argmax(predictions,axis=1))\n","print('y_testHot:', np.argmax(y_testHot,axis=1))"]},{"cell_type":"markdown","metadata":{},"outputs":[],"source":["# Predict"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"predictions: W\ny_label: W\n"}],"source":["# def read_img_V(image_path):\n","#     img = load_img(image_path, target_size=(75, 75))\n","#     image = img_to_array(img)\n","#     image = np.expand_dims(image, axis=0)\n","#     image = preprocess_input_(image)\n","#     return image\n","\n","\n","def read_image(image_path):\n","    img_file = cv2.imread(image_path)\n","    if img_file is not None:\n","        img_file = skimage.transform.resize(img_file, (imageSize, imageSize, 3))\n","        img_arr = np.asarray(img_file).reshape((-1, imageSize, imageSize, 3))\n","        \n","        return img_arr\n","\n","CHAR= 'W'\n","test_path='data/input/NewData/test/'+CHAR+'/'+CHAR+'32.jpg'\n","img=read_image(test_path)\n","predictions = model.predict(img)\n","print('predictions:', CLASSES[(np.argmax(predictions,axis=1))[0]])\n","print('y_label:', CHAR)\n"]},{"cell_type":"code","metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.7.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}